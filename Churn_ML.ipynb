{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "f5114d97-3303-4085-a2a1-d8a6c2ed1a5a",
      "cell_type": "code",
      "source": "%pip install seaborn\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8d79ae06-5617-4470-9d10-2f915531e426",
      "cell_type": "markdown",
      "source": "Let's load the file and make some checks",
      "metadata": {}
    },
    {
      "id": "afaa0766-dde9-41a1-95f5-d796304da4fb",
      "cell_type": "code",
      "source": "df = pd.read_csv(\"churn.csv\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "81a56e19-0a2f-40a7-828c-aa87bc71aebe",
      "cell_type": "code",
      "source": "df.head()",
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9eee2d1d-ac43-436f-8f31-231a07b44e42",
      "cell_type": "code",
      "source": "df.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "77b981c7-ec76-4b9c-ad23-4ddc523af3b8",
      "cell_type": "code",
      "source": "df.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "667ade27-20ff-4c1f-8236-dee192574f3f",
      "cell_type": "code",
      "source": "df.isna().sum()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2f4d772-0cd2-4b57-8bf1-e8cedff2edc8",
      "cell_type": "code",
      "source": "df[df.duplicated()]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a764b93-d03b-43fd-b098-38de032e24c8",
      "cell_type": "code",
      "source": "df.columns\nprint('Number of unique values in each column:')\nfor label in df.columns:\n        print(f'{label}: {df[label].nunique()}')",
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bc6ca250-b63e-4c66-bec7-96faab120bf4",
      "cell_type": "markdown",
      "source": "Let's drop the RowNumber, CustomerId, Surname columns as they are irrelevant to the analysis",
      "metadata": {}
    },
    {
      "id": "7b2f02bf-da7b-4599-a709-c916f02a2f3b",
      "cell_type": "code",
      "source": "df.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\"], inplace=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d1486539-82aa-40ed-a0a6-877aae81fd19",
      "cell_type": "code",
      "source": "df.head()",
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cc1c4ca8-df0a-4fa4-899d-d81a339a3ed3",
      "cell_type": "markdown",
      "source": "The dataset is clean, let's see what columns are numerical vs categorical",
      "metadata": {}
    },
    {
      "id": "2ae7d007-507f-4566-8068-7c82dabec8a5",
      "cell_type": "code",
      "source": "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\ncategorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"Numerical Columns:\", numerical_cols)\nprint(\"Categorical Columns:\", categorical_cols)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0ed18b05-e035-476c-bb57-ad009f51a071",
      "cell_type": "code",
      "source": "sns.countplot(data=df, x='Exited', palette = \"colorblind\", hue = \"Exited\");",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4f92bb5d-ab69-4c08-90ca-5df48cd72198",
      "cell_type": "markdown",
      "source": "Let's show the percentages ",
      "metadata": {}
    },
    {
      "id": "730e8eda-404b-4495-938d-584de993cdbe",
      "cell_type": "code",
      "source": "ax = sns.countplot(x='Exited', data=df, palette='colorblind', hue='Exited')\n\n# get the total count of the type column\ntotal = df['Exited'].count()\n\n# annotate the bars \nfor c in ax.containers:\n    ax.bar_label(c, fmt=lambda x: f'{(x/total)*100:0.1f}%')\n\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eabe7571-4364-4a04-a8e3-e4e52ab85303",
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(2, 3, figsize=(16, 10))\n\nsns.countplot(data=df, x='Geography', hue='Exited', ax=ax[0][0])\nax[0][0].set_title('Geography and Churn')\nsns.countplot(data=df, x='Gender', hue='Exited', ax=ax[0][1])\nax[0][1].set_title('Gender and Churn')\nsns.countplot(data=df, x='Tenure', hue='Exited', ax=ax[0][2])\nax[0][2].set_title('Tenure amnd Churn')\nsns.countplot(data=df, x='NumOfProducts', hue='Exited', ax=ax[1][0])\nax[1][0].set_title('Number of products and Churn')\nsns.countplot(data=df, x='HasCrCard', hue='Exited', ax=ax[1][1])\nax[1][1].set_title('Credit Card and Churn')\nsns.countplot(data=df, x='IsActiveMember', hue='Exited', ax=ax[1][2]);\nax[1][2].set_title('Activity and Churn')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "526f9161-3280-4d86-8902-448d324b7aa9",
      "cell_type": "markdown",
      "source": "People with a smaller amount of Products and people that are not active members are more likely to churn",
      "metadata": {}
    },
    {
      "id": "e913d864-6a88-4326-9c44-15738ee6aea1",
      "cell_type": "markdown",
      "source": "Let's create some histograms for the numeric values with a bigger range ",
      "metadata": {}
    },
    {
      "id": "f6d97c87-7f13-4bb5-96a2-bce5cc44217d",
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(2, 2, figsize=(16, 10))\n\nsns.histplot(data=df, x='Balance', hue='Exited', ax=ax[0][0])\nax[0][0].set_title('Balance Distribution by Churn')\nsns.histplot(data=df, x='Age', hue='Exited', ax=ax[0][1])\nax[0][1].set_title('Age Distribution by Churn')\nsns.histplot(data=df, x='CreditScore', hue='Exited', ax=ax[1][0])\nax[1][0].set_title('Credit Score Distribution by Churn')\nsns.histplot(data=df, x='EstimatedSalary', hue='Exited', ax=ax[1][1])\nax[1][1].set_title('Estimated Salary Distribution by Churn')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4de2aeea-119c-4e1a-b577-2349f1624231",
      "cell_type": "markdown",
      "source": "Customers with lower balance seem more likely to churn.\nAge seems like a normal distribution as a percentage of churners with the center at 50 but most of the customers are under 40 years old.",
      "metadata": {}
    },
    {
      "id": "58a6cdd8-00be-4f87-8fe3-fe8ea00c803e",
      "cell_type": "code",
      "source": "df = pd.get_dummies(df, columns=[\"Geography\"])\ndf[\"Gender\"] = df[\"Gender\"].map({'Female': 0, 'Male': 1})",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9a966d3c-a462-432a-978c-5d2e771cb78a",
      "cell_type": "code",
      "source": "df.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a7dbf5dd-d6f0-40e3-852c-951f4c6c4b86",
      "cell_type": "markdown",
      "source": "Since we have only three countries we can split them into three columns and make gender 0 for female and 1 for male",
      "metadata": {}
    },
    {
      "id": "e53051dc-2099-4800-b453-bd2c76bdd002",
      "cell_type": "code",
      "source": "plt.figure(figsize=(10, 6))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Heatmap of the Data')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "96cea771-62e3-4137-a280-97d805eb2ae9",
      "cell_type": "code",
      "source": "corr_results = df.corrwith(df['Exited']).abs().sort_values(ascending=False)\ncorr_results",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2a40b2fb-7411-4598-8354-0f471fe722c1",
      "cell_type": "markdown",
      "source": "Age, IsActiveMember, Balance along with Gender and Geography seem to correlate.",
      "metadata": {}
    },
    {
      "id": "69ae5d3c-0880-4777-8d9f-422d6a18f194",
      "cell_type": "markdown",
      "source": "Now let's try to do some feature engineering.",
      "metadata": {}
    },
    {
      "id": "88403195-8d19-4668-9443-eeb015bbbf18",
      "cell_type": "code",
      "source": "#Create a new dataframe and keeping the old one just in case.\ndf_enriched = df.copy()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c48ada3d-f1c5-4639-9dba-022967535258",
      "cell_type": "markdown",
      "source": "Let's put Age, Credit score, balance and estimated salary into groups.\nWe could define our own ranges and it would be better from a business perspective as each group would be more meaningful and not based on statistics. For example ages 18-28,26-36 etc instead of using quantile bins but we will keep it like this for now.",
      "metadata": {}
    },
    {
      "id": "f134db0f-49a2-4b6c-8db0-27e88a9f67c4",
      "cell_type": "code",
      "source": "df_enriched['Age_Groups'] = pd.qcut(df_enriched['Age'], 6, labels = [1, 2, 3, 4, 5, 6])\ndf_enriched[\"CreditsScore_Groups\"] = pd.qcut(df_enriched['CreditScore'], 8, labels = [1, 2, 3, 4, 5, 6,7,8])\ndf_enriched[\"Balance_score_groups\"] = pd.qcut(df_enriched['Balance'].rank(method=\"first\"), 5, labels = [1, 2, 3, 4, 5]) #rank needed because of many 0s\ndf_enriched[\"EstSalaryScore_groups\"] = pd.qcut(df_enriched['EstimatedSalary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8c8fef7c-2c85-436b-8fdb-9cad000bd9fd",
      "cell_type": "markdown",
      "source": "Let's create some more columns.",
      "metadata": {}
    },
    {
      "id": "2e99c553-8ed3-4ed3-8b98-3b5d07dda01b",
      "cell_type": "code",
      "source": "df_enriched[\"BalanceSalaryRatio\"] = df_enriched[\"Balance\"] / (df_enriched[\"EstimatedSalary\"] + 1)\ndf_enriched['Customer_Lifetime_Percentage'] = (df_enriched['Tenure'] / df_enriched['Age']) * 100\ndf_enriched[\"HighSalary\"] = (df_enriched[\"EstimatedSalary\"] > df_enriched[\"EstimatedSalary\"].median()).astype(int)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2321848b-4beb-4469-8d87-a5052cb4c589",
      "cell_type": "code",
      "source": "df_enriched.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "00986593-3168-488f-ad58-37acf0df5fe3",
      "cell_type": "markdown",
      "source": "We know we have an imbalanced dataset - most customers are not exiting.",
      "metadata": {}
    },
    {
      "id": "47c378a3-cda2-44be-a469-7e26874f078e",
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX = df_enriched.drop('Exited', axis=1)\ny = df_enriched['Exited']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5ac66f82-bf9d-43ed-b3d4-68dcd0dc503b",
      "cell_type": "markdown",
      "source": "We could use class weights, like we do here, under sample the majority class or oversample the minority class.",
      "metadata": {}
    },
    {
      "id": "4bd4a2e0-ac07-4d4d-8d99-1b3efdca7231",
      "cell_type": "markdown",
      "source": "Let's start training some models.",
      "metadata": {}
    },
    {
      "id": "7b082823-fde9-448c-a58c-3756ef773292",
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(class_weight='balanced', random_state=42)\nclf.fit(X_train, y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bbe2b92c-58bd-45d6-b86a-bdf2fee3ba7b",
      "cell_type": "code",
      "source": "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n\ny_pred = clf.predict(X_test)\ny_proba = clf.predict_proba(X_test)[:, 1]\nprint(classification_report(y_test, y_pred))\n\nroc_auc_rf = roc_auc_score(y_test, y_proba)\nprint(\"ROC-AUC:\", roc_auc)\n\n# ROC Curve\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_proba)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_rf, tpr_rf, label=f\"ROC Curve (AUC = {roc_auc_rf:.3f})\")\nplt.plot([0, 1], [0, 1], linestyle='--') \nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve - Random Forest\")\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e0fe2b6c-f08f-494f-96ac-006a3ed948b7",
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(n_estimators=300,learning_rate=0.05,max_depth=3,random_state=42)\ngb.fit(X_train, y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "14439ce8-0da2-48b0-8d4f-fd50f1e03dd2",
      "cell_type": "code",
      "source": "y_pred_gb = gb.predict(X_test)\ny_proba_gb = gb.predict_proba(X_test)[:, 1]\n\nprint(classification_report(y_test, y_pred_gb))\nroc_auc_gb = roc_auc_score(y_test, y_proba_gb)\nprint(\"ROC-AUC:\", roc_auc_gb)\n\nfpr_gb, tpr_gb, _ = roc_curve(y_test, y_proba_gb)\n\n#ROC Curve\nfpr_gb, tpr_gb, thresholds_gb = roc_curve(y_test, y_proba_gb)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_gb, tpr_gb, label=f\"ROC Curve (AUC = {roc_auc_gb:.3f})\")\nplt.plot([0, 1], [0, 1], linestyle='--') \nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve - Gradient Boosting\")\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4e827c7f-13f9-4412-b659-49a7a125e39c",
      "cell_type": "code",
      "source": "plt.figure(figsize=(8, 6))\nplt.plot(fpr_rf, tpr_rf, label=f\"Random Forest ROC (AUC={roc_auc_rf:.3f})\")\nplt.plot(fpr_gb, tpr_gb, label=f\"Gradient Boosting ROC (AUC={roc_auc_gb:.3f})\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve Comparison - Random Forest vs Gradient Boosting\")\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a9b96404-a938-440c-b791-aecfa63dc754",
      "cell_type": "code",
      "source": "# Random Forest\nfi_rf = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf.feature_importances_}).sort_values('Importance', ascending=False)\n# Gradient Boosting\nfi_gb = pd.DataFrame({'Feature': X_train.columns, 'Importance': gb.feature_importances_}).sort_values('Importance', ascending=False)\n\nprint(\"=== Random Forest Feature Importance ===\")\nprint(fi_rf.head(10))\nprint(\"\\n=== Gradient Boosting Feature Importance ===\")\nprint(fi_gb.head(10))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b945431b-e694-4dc6-99a8-53b93857339f",
      "cell_type": "markdown",
      "source": "Some of the same features seem to be important for both models.",
      "metadata": {}
    },
    {
      "id": "55437f6d-9ccc-4fbf-b986-f7bd2567ddb9",
      "cell_type": "markdown",
      "source": "Let's change our threshold - we want to maximize recall and catch as many churners as possible. False positives will increase but are a lot less important than not catching a churner.",
      "metadata": {}
    },
    {
      "id": "8251b801-f88d-4efc-a533-394edbec1ea3",
      "cell_type": "code",
      "source": "from sklearn.metrics import precision_recall_fscore_support\n\ndef threshold_metrics(y_true, y_proba, thresholds):\n    \"\"\"\n    Computes precision, recall, f1 for a range of probability thresholds.\n    \"\"\"\n    results = []\n    for t in thresholds:\n        y_pred = (y_proba >= t).astype(int)\n        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n        results.append((t, precision, recall, f1))\n    return np.array(results)\n\n# Define thresholds\nthresholds = np.arange(0.1, 0.6, 0.05)\n\n# Random Forest\nrf_results = threshold_metrics(y_test, y_proba_rf, thresholds)\n\n# Gradient Boosting\ngb_results = threshold_metrics(y_test, y_proba_gb, thresholds)\n\n# Display\nprint(\"=== Random Forest Threshold Optimization ===\")\nfor t, p, r, f1 in rf_results:\n    print(f\"Threshold {t:.2f}: Precision={p:.3f}, Recall={r:.3f}, F1={f1:.3f}\")\n\nprint(\"\\n=== Gradient Boosting Threshold Optimization ===\")\nfor t, p, r, f1 in gb_results:\n    print(f\"Threshold {t:.2f}: Precision={p:.3f}, Recall={r:.3f}, F1={f1:.3f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a69af0f8-1027-4fbc-b684-b2d445c4450f",
      "cell_type": "markdown",
      "source": "For both models the best threholds are at around 0.25 - 0.3",
      "metadata": {}
    },
    {
      "id": "c9c947fe-d7dc-47ab-9661-037ab4b0afed",
      "cell_type": "code",
      "source": "best_threshold = 0.25 \ny_pred_final = (y_proba_gb >= best_threshold).astype(int)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e7c075ab-3ed0-4fb1-b7ad-efdc13480f8d",
      "cell_type": "code",
      "source": "gb_tuned = GradientBoostingClassifier(n_estimators=500,learning_rate=0.03,max_depth=5,subsample=0.9,max_features=0.8,random_state=42)\ngb_tuned.fit(X_train, y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0b79b672-bec7-4dbe-838e-b3466d9dffc6",
      "cell_type": "code",
      "source": "y_proba_gb_tuned = gb_tuned.predict_proba(X_test)[:, 1]\ny_pred_gb_tuned = (y_proba_gb_tuned >= 0.25).astype(int)\n\n\nprint(classification_report(y_test, y_pred_gb_tuned))\nroc_auc_gb_tuned = roc_auc_score(y_test, y_proba_gb_tuned)\nprint(\"ROC-AUC:\", roc_auc_gb_tuned)\n\nfpr_gb_tuned, tpr_gb_tuned, _ = roc_curve(y_test, y_proba_gb_tuned)\n\n#ROC Curve\nfpr_gb_tuned, tpr_gb_tuned, thresholds_gb_tuned = roc_curve(y_test, y_proba_gb_tuned)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_gb_tuned, tpr_gb_tuned, label=f\"ROC Curve (AUC = {roc_auc_gb_tuned:.3f})\")\nplt.plot([0, 1], [0, 1], linestyle='--') \nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve - Gradient Boosting Tuned\")\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "caf99bef-7031-4e26-99e5-c4627e419912",
      "cell_type": "markdown",
      "source": "Recall for churners has increased to 0.70 from 0.49 but precision has dropped. This is an acceptable trade-off since missing a churner is worse senario that falsely flagging one.",
      "metadata": {}
    },
    {
      "id": "de5df480-c950-4b6b-9d08-c7f2547e6048",
      "cell_type": "code",
      "source": "import joblib\n\n# Save Random Forest\njoblib.dump(clf, \"random_forest_churn.pkl\")\n\n# Save Gradient Boosting\njoblib.dump(gb, \"gradient_boosting_churn.pkl\")\n\n# Save Tuned Gradient Boosting\njoblib.dump(gb_tuned, \"gradient_boosting_tuned_churn.pkl\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}